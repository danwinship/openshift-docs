[[install-config-configuring-sdn]]
= Configuring the SDN
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

The xref:../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[{product-title} SDN] enables
communication between pods across the {product-title} cluster, establishing a _pod
network_. Two xref:../architecture/additional_concepts/sdn.adoc#architecture-additional-concepts-sdn[SDN plug-ins]
are currently available (*ovs-subnet* and *ovs-multitenant*), which provide
different methods for configuring the pod network.

[[configuring-sdn-config-pod-network-ansible]]
== Configuring the Pod Network with Ansible

For initial xref:../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installations],
the *ovs-subnet* plug-in is installed and configured by default, though it can
be
xref:../install_config/install/advanced_install.adoc#configuring-ansible[overridden during installation]
using the
xref:../install_config/install/advanced_install.adoc#configuring-cluster-variables[`*os_sdn_network_plugin_name*` parameter],
which is configurable in the Ansible inventory file.

.Example SDN Configuration with Ansible
====

----
# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
# os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

# Disable the OpenShift SDN plugin
# openshift_use_openshift_sdn=False

# Configure SDN cluster network CIDR block. This network block should
# be a private block and should not conflict with existing network
# blocks in your infrastructure that pods may require access to.
# Can not be changed after deployment.
#osm_cluster_network_cidr=10.1.0.0/16

# default subdomain to use for exposed routes
#openshift_master_default_subdomain=apps.test.example.com

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
#osm_cluster_network_cidr=10.1.0.0/16
#openshift_portal_net=172.30.0.0/16

# Configure number of bits to allocate to each hostâ€™s subnet e.g. 8
# would mean a /24 network on the host.
#osm_host_subnet_length=8

# This variable specifies the service proxy implementation to use:
# either iptables for the pure-iptables version (the default),
# or userspace for the userspace proxy.
#openshift_node_proxy_mode=iptables
----
====

ifdef::openshift-enterprise[]
For initial xref:../install_config/install/quick_install.adoc#install-config-install-quick-install[quick installations],
the *ovs-subnet* plug-in is installed and configured by default as well, and can
be
xref:../install_config/master_node_configuration.adoc#master-configuration-files[reconfigured post-installation]
using the `*networkConfig*` stanza of the *_master-config.yaml_* file.
endif::[]

[[configuring-the-pod-network-on-masters]]
== Configuring the Pod Network on Masters

Cluster administrators can control pod network settings on masters by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[master configuration file]
(located at *_/etc/origin/master/master-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  clusterNetworkCIDR: 10.128.0.0/14 <1>
  hostSubnetLength: 9 <2>
  networkPluginName: "redhat/openshift-ovs-subnet" <3>
  serviceNetworkCIDR: 172.30.0.0/16 <4>
----
<1> Cluster network for node IP allocation
<2> Number of bits for pod IP allocation within a node
<3> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in or
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in
<4> Service IP allocation for the cluster
====

[IMPORTANT]
====
The `*serviceNetworkCIDR*` and `*hostSubnetLength*` values cannot be changed
after the cluster is first created, and `*clusterNetworkCIDR*` can only be
changed to be a larger network that still contains the original network. For
example, given the default value of *10.128.0.0/14*, you could change
`*clusterNetworkCIDR*` to *10.128.0.0/9* (i.e., the entire upper half of net
10) but not to *10.64.0.0/16*, because that does not overlap the original value.
====

[[configuring-the-pod-network-on-nodes]]
== Configuring the Pod Network on Nodes

Cluster administrators can control pod network settings on nodes by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[node configuration file]
(located at *_/etc/origin/node/node-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  mtu: 1450 <1>
  networkPluginName: "redhat/openshift-ovs-subnet" <2>
----
<1> Maximum transmission unit (MTU) for the pod overlay network
<2> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in or
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in
====

[[migrating-between-sdn-plugins]]
== Migrating Between SDN Plug-ins

If you are already using one SDN plug-in and want to switch to another:

. Change the `*networkPluginName*` parameter on all
xref:configuring-the-pod-network-on-masters[masters] and
xref:configuring-the-pod-network-on-nodes[nodes] in their configuration files.
ifdef::openshift-origin[]
. Restart the *origin-master* service on masters and the *origin-node* service
on nodes.
endif::[]
ifdef::openshift-enterprise[]
. Restart the *atomic-openshift-master* service on masters and the
*atomic-openshift-node* service on nodes.
endif::[]
. If you are switching from an {product-title} SDN plug-in to a
third-party plug-in, then clean up {product-title} SDN-specific
artifacts:
----
$ oc delete clusternetwork --all
$ oc delete hostsubnets --all
$ oc delete netnamespaces --all
----

When switching from the *ovs-subnet* to the *ovs-multitenant* {product-title} SDN plug-in,
all the existing projects in the cluster will be fully isolated (assigned unique VNIDs).
Cluster administrators can choose to xref:../admin_guide/managing_pods.adoc#admin-guide-pod-network[modify
the project networks] using the administrator CLI.

Check VNIDs by running:

----
$ oc get netnamespace
----

[[renumbering-sdn-networks]]
== Renumbering the SDN Network

It is difficult to change the networking configuration of an
{product-title} cluster after it has been deployed, but if absolutely
necessary, it can be done.

=== Expanding the Cluster IP Range By Adding Nodes

The easiest reconfiguration involves changing the
`*clusterNetworkCIDR*` to make room to add additional nodes, while
leaving existing Nodes and Pods untouched. (That is, changing
`*clusterNetworkCIDR*` to a new value which is larger than the old
value, and which includes the old value as a subset.)

For example, with the default `*clusterNetworkCIDR*` value of
*10.128.0.0/14* and the default `*hostSubnetLength*` of *9*, you can
have 512 nodes, with Pod subnets from *10.128.0.0/23* to
*10.131.254.0/23*. (Older versions of {product-title} had smaller
defaults.) If you changed `*clusterNetworkCIDR*` to *10.128.0.0/13*,
then you could have an additional 512 nodes, with Pod IPs going up to
*10.135.255.255*.

To do this, simply change the `*clusterNetworkCIDR*` value in the
`*networkConfig*` stanza of the *_master-config.yaml_* file, and then
restart the master; for single master clusters:

----
# systemctl restart atomic-openshift-master
----

For multi-master clusters, on each master:

----
# systemctl restart atomic-openshift-master-controllers
----

You will also need to restart the node service on the master and on
each of the nodes, to make them update their IP routing information to
take the larger cluster network into account:

----
# systemctl restart atomic-openshift-node
----

As stated above, this only works when the new `*clusterNetworkCIDR*`
value fully contains the original value; if you try to change it to
any other value, then when you restart the master, it will complain
about the mismatch and refuse to start. You will need to use the
procedure below in this case.

=== Other Changes to the Cluster Network IP Range

If you want to completely change `*clusterNetworkCIDR*` (eg, moving
from *10.128.0.0/14* to *192.168.9.0/8*) or you want to change
`*hostSubnetLength*` (to allow more or fewer pods per node), you will
need to completely take down the cluster temporarily.

[WARNING]
You should xref:../admin_guide/backup_restore.adoc[back up] your
cluster before attempting this procedure.

. On every node and master, do:
+
----
# systemctl stop atomic-openshift-node
# systemctl restart docker
# ovs-vsctl del-br br0
----

. As a cluster admin user, run:
+
----
$ oc delete hostsubnets --all
$ oc delete clusternetwork default
----

. If you are reconfiguring your network to be _smaller_, and need to
get rid of some of your existing nodes, delete the excess node records
now. e.g.:
+
----
$ oc delete node node-33
$ oc delete node node-34
$ oc delete node node-35
----

. Stop the masters. In a single-master cluster:
+
----
# systemctl stop atomic-openshift-master
----
+
For multi-master clusters, on each master:
+
----
# systemctl stop atomic-openshift-master-controllers
# systemctl stop atomic-openshift-master-api
----

. Now edit the *_master-config.yaml_* file and make the desired changes.

. Restart the masters:
+
----
# systemctl start atomic-openshift-master
----
+
or (multi-master):
+
----
# systemctl start atomic-openshift-master-controllers
# systemctl start atomic-openshift-master-api
----

. Restart the node service on all of the masters and (remaining) nodes:
+
----
# systemctl start atomic-openshift-node
----

At this point the cluster will be back up, but every
previously-running pod will have been killed, and it may take a few
minutes to get everything completely back up and running again.

=== Changing the Service Network IP Range

If you need to change `*serviceNetworkCIDR*`, either because you need
more than 65,536 service IPs, or because you have belatedly discovered
a conflict between the service IP range and other IPs on your network,
you will have to take down the cluster (as above), but you will also
need to delete and recreate all Service objects. There is no simple
way to do this, and in particular, to get all of the permissions
correct, each Service will have to be recreated by the correct user;
you can't simply have an administrator recreate them. Thus, this will
result in a more noticeable outage than the `*clusterNetworkCIDR*`
renumbering case.

[WARNING]
You should xref:../admin_guide/backup_restore.adoc[back up] your
cluster before attempting this procedure.

. On every node and master, do:
+
----
# systemctl stop atomic-openshift-node
# ovs-vsctl del-br br0
----

. Stop the masters. In a single-master cluster:
+
----
# systemctl stop atomic-openshift-master
----
+
For multi-master clusters, on each master:
+
----
# systemctl stop atomic-openshift-master-controllers
# systemctl stop atomic-openshift-master-api
----

. Now edit the *_master-config.yaml_* file and make the desired change
to `*serviceNetworkCIDR*` in the `*networkConfig*` section, and
`*servicesSubnet*` in the `*kubernetesMasterConfig*` section.
Additionally, comment out the `*networkPluginName*` line in
`*networkConfig*` so that {product-title} will start with no network
plugin next time.

. Now restart the masters:
+
----
# systemctl start atomic-openshift-master
----
+
or (multi-master):
+
----
# systemctl start atomic-openshift-master-controllers
# systemctl start atomic-openshift-master-api
----

. As a cluster admin user, delete all services in all namespaces, and
delete the ClusterNetwork record:
+
----
$ for ns in $(oc get namespaces -o jsonpath='{..name}'); do
    oc delete services -n $ns --all
  done
$ oc delete clusternetwork default
----

. And stop the masters again:
+
----
# systemctl start atomic-openshift-master
----
+
or (multi-master):
+
----
# systemctl start atomic-openshift-master-controllers
# systemctl start atomic-openshift-master-api
----

. Edit *_master-config.yaml_* again and uncomment the
`*networkPluginName*` line. Then start the masters again:
+
----
# systemctl restart atomic-openshift-master
----
+
or (multi-master):
+
----
# systemctl restart atomic-openshift-master-controllers
# systemctl restart atomic-openshift-master-api
----

. Restart the node service on all of the nodes and masters:
+
----
# systemctl start atomic-openshift-node
----

Unlike in the `*clusterNetworkCIDR*`-renumbering case, this will not
kill previously-running pods. However, users will have to recreate
their services manually after the renumbering since they were deleted
as part of the procedure.

[[external-access-to-the-cluster-network]]
== External Access to the Cluster Network

If a host that is external to {product-title} requires access to the cluster network,
you have two options:

. Configure the host as an {product-title} node but mark it
xref:../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[unschedulable]
so that the master does not schedule containers on it.
. Create a tunnel between your host and a host that is on the cluster network.

Both options are presented as part of a practical use-case in the documentation
for configuring xref:../install_config/routing_from_edge_lb.adoc#install-config-routing-from-edge-lb[routing from an
edge load-balancer to containers within {product-title} SDN].
